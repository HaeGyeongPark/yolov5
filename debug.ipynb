{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "# Jupyter notebook for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "e8225db4-e61d-4640-8b1f-8bfce3331cea"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Copied from `train` function in train_simple.py:L78\n",
        "import yaml\n",
        "\n",
        "device = 'cpu'\n",
        "hyp = 'data/hyps/hyp.scratch-low.yaml'\n",
        "\n",
        "with open(hyp, errors=\"ignore\") as f:\n",
        "    hyp = yaml.safe_load(f)  # load hyps dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding model.yaml nc=4 with nc=15\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
            "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
            "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
            "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n",
            "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
            "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
            " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
            " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 24      [17, 20, 23]  1      9020  models.yolo.Detect                      [15, [[17, 17], [64, 39], [94, 131]], [64, 128, 256]]\n",
            "YOLOv5n_nuscenes summary: 214 layers, 1766172 parameters, 1766172 gradients, 4.2 GFLOPs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from models.yolo import Model\n",
        "from utils.general import check_dataset\n",
        "\n",
        "cfg = 'models/yolov5n_nuscenes.yaml'\n",
        "data = 'data/nuscenes.yaml'\n",
        "data_dict = check_dataset(data)\n",
        "\n",
        "nc = int(data_dict[\"nc\"])  # number of classes\n",
        "model = Model(cfg, ch=3, nc=nc, anchors=hyp.get(\"anchors\")).to(device)  # create"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchors = model.model[-1].anchors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [TODO] Draw anchors\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "anchors_arr = anchors.numpy().reshape(-1, 2)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "max_width = max(rect[0] for rect in anchors_arr)\n",
        "max_height = max(rect[1] for rect in anchors_arr)\n",
        "\n",
        "for width, height in anchors_arr:\n",
        "    lower_left_corner = (-width / 2, -height / 2)\n",
        "    rect = patches.Rectangle(lower_left_corner, width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "ax.set_xlim(-max_width / 2 - 1, max_width / 2 + 1)\n",
        "ax.set_ylim(-max_height / 2 - 1, max_height / 2 + 1)\n",
        "ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/yolov5/AUE8088-PA2/datasets/nuscenes/train.cache... 28130 images, 1425 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28130/28130 [00:00<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "from utils.dataloaders import create_dataloader\n",
        "from utils.general import check_img_size, colorstr\n",
        "\n",
        "imgsz = 416\n",
        "batch_size = 1\n",
        "single_cls = False\n",
        "seed = 0\n",
        "\n",
        "train_path = data_dict[\"train\"]\n",
        "gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
        "imgsz = check_img_size(imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple\n",
        "\n",
        "train_loader, dataset = create_dataloader(\n",
        "    train_path,\n",
        "    imgsz,\n",
        "    batch_size,\n",
        "    gs,\n",
        "    single_cls,\n",
        "    hyp=hyp,\n",
        "    augment=True,\n",
        "    cache=None,\n",
        "    rect=False,\n",
        "    rank=-1,\n",
        "    workers=8,\n",
        "    image_weights=False,\n",
        "    quad=False,\n",
        "    prefix=colorstr(\"train: \"),\n",
        "    shuffle=True,\n",
        "    seed=seed,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "for imgs, targets, paths, _ in train_loader:\n",
        "    imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 ðŸš€ ddf4e91 Python-3.10.12 torch-2.3.1+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.torch_utils import select_device\n",
        "\n",
        "weights = 'yolov5n.pt'\n",
        "# data = 'data/nuscenes.yaml'\n",
        "data = 'data/nuscences.yaml'\n",
        "half = False  # use FP16 half-precision inference\n",
        "dnn = False  # use OpenCV DNN for ONNX inference\n",
        "device = select_device('cpu')\n",
        "\n",
        "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
        "\n",
        "# inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(imgs)  # forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.general import non_max_suppression\n",
        "\n",
        "conf_thres = 0.25  # confidence threshold\n",
        "iou_thres = 0.45  # NMS IOU threshold\n",
        "max_det = 1000  # maximum detections per image\n",
        "classes = None\n",
        "agnostic_nms = False  # class-agnostic NMS\n",
        "\n",
        "pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'seen' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# [TODO] draw predictions (see detect.py:L178)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, det \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pred):  \u001b[38;5;66;03m# per image\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m             seen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m webcam:  \u001b[38;5;66;03m# batch_size >= 1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                 p, im0, frame \u001b[38;5;241m=\u001b[39m path[i], im0s[i]\u001b[38;5;241m.\u001b[39mcopy(), dataset\u001b[38;5;241m.\u001b[39mcount\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seen' is not defined"
          ]
        }
      ],
      "source": [
        "# [TODO] draw predictions (see detect.py:L178)\n",
        "for i, det in enumerate(pred):  # per image\n",
        "            seen += 1\n",
        "            if webcam:  # batch_size >= 1\n",
        "                p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
        "                s += f\"{i}: \"\n",
        "            else:\n",
        "                p, im0, frame = path, im0s.copy(), getattr(dataset, \"frame\", 0)\n",
        "\n",
        "            p = Path(p)  # to Path\n",
        "            save_path = str(save_dir / p.name)  # im.jpg\n",
        "            txt_path = str(save_dir / \"labels\" / p.stem) + (\"\" if dataset.mode == \"image\" else f\"_{frame}\")  # im.txt\n",
        "            s += \"%gx%g \" % im.shape[2:]  # print string\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
        "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, 5].unique():\n",
        "                    n = (det[:, 5] == c).sum()  # detections per class\n",
        "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    c = int(cls)  # integer class\n",
        "                    label = names[c] if hide_conf else f\"{names[c]}\"\n",
        "                    confidence = float(conf)\n",
        "                    confidence_str = f\"{confidence:.2f}\"\n",
        "\n",
        "                    if save_csv:\n",
        "                        write_to_csv(p.name, label, confidence_str)\n",
        "\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "                        with open(f\"{txt_path}.txt\", \"a\") as f:\n",
        "                            f.write((\"%g \" * len(line)).rstrip() % line + \"\\n\")\n",
        "\n",
        "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
        "                        c = int(cls)  # integer class\n",
        "                        label = None if hide_labels else (names[c] if hide_conf else f\"{names[c]} {conf:.2f}\")\n",
        "                        annotator.box_label(xyxy, label, color=colors(c, True))\n",
        "                    if save_crop:\n",
        "                        save_one_box(xyxy, imc, file=save_dir / \"crops\" / names[c] / f\"{p.stem}.jpg\", BGR=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "YOLOv5 Tutorial",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
